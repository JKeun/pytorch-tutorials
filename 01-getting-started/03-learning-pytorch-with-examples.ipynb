{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEARNING PYTORCH WITH EXAMPLES\n",
    "This tutorial introdueces the fundamental concepts of [Py-Torch](https://github.com/pytorch/pytorch) through self-contained examples.\n",
    "\n",
    "At its core, PyTorch provides two main features:\n",
    "- An n -dimensional Tensor, similar to numpy but ca run on GPUs\n",
    "- Automatic differentiation for building and training neural networks\n",
    "\n",
    "We will use a fully-connected ReLU network as our running example. The network will have a single hidden layer, and will be trained with gradient descent to fit random data by minimizing the Euclidean distance between the network output and the true output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "### Warm-up: numpy\n",
    "Before introducing PyTroch, we will first implement the network using numpy.\n",
    "\n",
    "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays. Numpy is a generic gramework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a two-layer network to random data by manually implementing the forward and backward passes through the network using numpy operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.50307088, -0.15185053, -0.34081556, -0.30341894],\n",
       "       [ 0.54715363,  0.33065336,  0.31636158, -0.69556258],\n",
       "       [-0.86570228,  0.19769972,  1.10967712,  0.48293151]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 28039873.966130495)\n",
      "(1, 20472276.079896238)\n",
      "(2, 16512349.60131437)\n",
      "(3, 13586239.235681716)\n",
      "(4, 10938049.854629451)\n",
      "(5, 8470256.300047835)\n",
      "(6, 6319554.249940729)\n",
      "(7, 4593580.153910894)\n",
      "(8, 3307110.435857486)\n",
      "(9, 2392634.040389738)\n",
      "(10, 1761199.4217992234)\n",
      "(11, 1326971.1552512283)\n",
      "(12, 1026653.9182247353)\n",
      "(13, 815183.5272028478)\n",
      "(14, 662441.0549544694)\n",
      "(15, 548962.3566950667)\n",
      "(16, 462358.38123568974)\n",
      "(17, 394424.4327615809)\n",
      "(18, 339878.5554095361)\n",
      "(19, 295217.94464816886)\n",
      "(20, 258042.41865338816)\n",
      "(21, 226735.26874670837)\n",
      "(22, 200091.2773070959)\n",
      "(23, 177241.97131199512)\n",
      "(24, 157605.74365220417)\n",
      "(25, 140540.88703333965)\n",
      "(26, 125669.53303626839)\n",
      "(27, 112638.42396724306)\n",
      "(28, 101162.08790136671)\n",
      "(29, 91036.8585282597)\n",
      "(30, 82071.03383399942)\n",
      "(31, 74113.72695723925)\n",
      "(32, 67034.92822275579)\n",
      "(33, 60725.87531058881)\n",
      "(34, 55091.57766087088)\n",
      "(35, 50047.17976619336)\n",
      "(36, 45525.93840649478)\n",
      "(37, 41477.18382123007)\n",
      "(38, 37837.17774789438)\n",
      "(39, 34555.98910978215)\n",
      "(40, 31593.059126752247)\n",
      "(41, 28915.468383598185)\n",
      "(42, 26492.262351915062)\n",
      "(43, 24295.74711202844)\n",
      "(44, 22302.469377056375)\n",
      "(45, 20491.290785879828)\n",
      "(46, 18843.963087020747)\n",
      "(47, 17342.136380725788)\n",
      "(48, 15973.112509076018)\n",
      "(49, 14723.231289152744)\n",
      "(50, 13580.730143383056)\n",
      "(51, 12536.648288830822)\n",
      "(52, 11581.075966535758)\n",
      "(53, 10705.506595741681)\n",
      "(54, 9902.451632712608)\n",
      "(55, 9165.020851617099)\n",
      "(56, 8488.245946450581)\n",
      "(57, 7865.490775276295)\n",
      "(58, 7292.38791173206)\n",
      "(59, 6765.215389859575)\n",
      "(60, 6279.262612049279)\n",
      "(61, 5831.032457701255)\n",
      "(62, 5417.328365240704)\n",
      "(63, 5035.422120321857)\n",
      "(64, 4682.625316727723)\n",
      "(65, 4356.227820752978)\n",
      "(66, 4054.388635453945)\n",
      "(67, 3774.899997404957)\n",
      "(68, 3516.0652990267954)\n",
      "(69, 3276.3555924245434)\n",
      "(70, 3054.085743707196)\n",
      "(71, 2847.900035560262)\n",
      "(72, 2656.593607252232)\n",
      "(73, 2479.027039526899)\n",
      "(74, 2314.007323565562)\n",
      "(75, 2160.7023060683628)\n",
      "(76, 2018.1107820269456)\n",
      "(77, 1885.4965160111617)\n",
      "(78, 1762.1030138681162)\n",
      "(79, 1647.2556688975887)\n",
      "(80, 1540.3383964186073)\n",
      "(81, 1440.8151428499627)\n",
      "(82, 1347.9901665370878)\n",
      "(83, 1261.4823899161568)\n",
      "(84, 1180.796012735529)\n",
      "(85, 1105.5322899066766)\n",
      "(86, 1035.3147229968117)\n",
      "(87, 969.7964349400442)\n",
      "(88, 908.632970640283)\n",
      "(89, 851.5098862777548)\n",
      "(90, 798.1341169976894)\n",
      "(91, 748.2716703121557)\n",
      "(92, 701.6523919066632)\n",
      "(93, 658.0593182648747)\n",
      "(94, 617.301957539567)\n",
      "(95, 579.1895900531948)\n",
      "(96, 543.5166354484138)\n",
      "(97, 510.1443822088209)\n",
      "(98, 478.89697499462204)\n",
      "(99, 449.6468617264453)\n",
      "(100, 422.2489057923698)\n",
      "(101, 396.58690013431874)\n",
      "(102, 372.5500925163587)\n",
      "(103, 350.0151933068396)\n",
      "(104, 328.89795528406273)\n",
      "(105, 309.111391765483)\n",
      "(106, 290.54456296056026)\n",
      "(107, 273.1376979298778)\n",
      "(108, 256.81317667661784)\n",
      "(109, 241.4993940855175)\n",
      "(110, 227.12484809709935)\n",
      "(111, 213.6322823366995)\n",
      "(112, 200.97232618791642)\n",
      "(113, 189.09429016186306)\n",
      "(114, 177.92856161087082)\n",
      "(115, 167.44508493211373)\n",
      "(116, 157.59808127363493)\n",
      "(117, 148.3508785860834)\n",
      "(118, 139.66133158136296)\n",
      "(119, 131.49565650598234)\n",
      "(120, 123.81868475100546)\n",
      "(121, 116.60721970925442)\n",
      "(122, 109.82546214592048)\n",
      "(123, 103.45228558889625)\n",
      "(124, 97.45622958740998)\n",
      "(125, 91.81818847704477)\n",
      "(126, 86.51553512552591)\n",
      "(127, 81.52704279845675)\n",
      "(128, 76.83476390686545)\n",
      "(129, 72.42059564217854)\n",
      "(130, 68.26378940687385)\n",
      "(131, 64.3515461344715)\n",
      "(132, 60.669332856880374)\n",
      "(133, 57.20515682286221)\n",
      "(134, 53.941711545310966)\n",
      "(135, 50.86981374435875)\n",
      "(136, 47.97657405360576)\n",
      "(137, 45.25325275604757)\n",
      "(138, 42.68729744971767)\n",
      "(139, 40.26965286243837)\n",
      "(140, 37.99163950165872)\n",
      "(141, 35.845605153417836)\n",
      "(142, 33.82369127716314)\n",
      "(143, 31.918656924226372)\n",
      "(144, 30.124060948621402)\n",
      "(145, 28.43276267202106)\n",
      "(146, 26.837044729247765)\n",
      "(147, 25.333255343402)\n",
      "(148, 23.915179736180516)\n",
      "(149, 22.578170244846188)\n",
      "(150, 21.317453356075358)\n",
      "(151, 20.128837910567974)\n",
      "(152, 19.007947888835098)\n",
      "(153, 17.95082758637766)\n",
      "(154, 16.953256485250844)\n",
      "(155, 16.012468170267972)\n",
      "(156, 15.124731888091)\n",
      "(157, 14.28709223685786)\n",
      "(158, 13.496728227837623)\n",
      "(159, 12.751038814871883)\n",
      "(160, 12.047771720177067)\n",
      "(161, 11.383471964723187)\n",
      "(162, 10.756467180757866)\n",
      "(163, 10.164836497622707)\n",
      "(164, 9.606183232736122)\n",
      "(165, 9.078978570181523)\n",
      "(166, 8.581042380392402)\n",
      "(167, 8.110984568107266)\n",
      "(168, 7.667278968659818)\n",
      "(169, 7.247997282468796)\n",
      "(170, 6.852037794061853)\n",
      "(171, 6.478154194899495)\n",
      "(172, 6.124999928848349)\n",
      "(173, 5.791463462031853)\n",
      "(174, 5.476457195544172)\n",
      "(175, 5.1789553878710155)\n",
      "(176, 4.897772717836336)\n",
      "(177, 4.63204359572204)\n",
      "(178, 4.380945994076688)\n",
      "(179, 4.143705561829949)\n",
      "(180, 3.9195330982361485)\n",
      "(181, 3.707644932275123)\n",
      "(182, 3.507464274365436)\n",
      "(183, 3.3183478150698353)\n",
      "(184, 3.139431684323562)\n",
      "(185, 2.9703363930774938)\n",
      "(186, 2.8104856427058245)\n",
      "(187, 2.6594054400912874)\n",
      "(188, 2.5165996623012292)\n",
      "(189, 2.3815279552023676)\n",
      "(190, 2.253886764875891)\n",
      "(191, 2.133135539026359)\n",
      "(192, 2.018922300741001)\n",
      "(193, 1.9109339237334602)\n",
      "(194, 1.8088123713829876)\n",
      "(195, 1.7122211658497704)\n",
      "(196, 1.6208559298224827)\n",
      "(197, 1.534505083322153)\n",
      "(198, 1.4527760678130548)\n",
      "(199, 1.3754471399071375)\n",
      "(200, 1.3022909954989397)\n",
      "(201, 1.2331066424473283)\n",
      "(202, 1.1676319031104376)\n",
      "(203, 1.1056786115350667)\n",
      "(204, 1.0470955561836566)\n",
      "(205, 0.9916307876862767)\n",
      "(206, 0.9391303519126707)\n",
      "(207, 0.8894592549157933)\n",
      "(208, 0.8424458320539749)\n",
      "(209, 0.7979478097915811)\n",
      "(210, 0.755848635210459)\n",
      "(211, 0.7160108347267036)\n",
      "(212, 0.6782811930396425)\n",
      "(213, 0.6425729986385897)\n",
      "(214, 0.6087746856519958)\n",
      "(215, 0.5767670967709069)\n",
      "(216, 0.5464612911715149)\n",
      "(217, 0.5177727534135617)\n",
      "(218, 0.4906239424833433)\n",
      "(219, 0.4649011388078853)\n",
      "(220, 0.44054797943637347)\n",
      "(221, 0.41748690187426757)\n",
      "(222, 0.39564811766524954)\n",
      "(223, 0.37496932629448787)\n",
      "(224, 0.3553817989164404)\n",
      "(225, 0.3368383577145258)\n",
      "(226, 0.31927158853907084)\n",
      "(227, 0.30263031175071)\n",
      "(228, 0.28686525573461324)\n",
      "(229, 0.27193176665771746)\n",
      "(230, 0.25778724280359533)\n",
      "(231, 0.2443851880217085)\n",
      "(232, 0.2316947451844453)\n",
      "(233, 0.21967044223204973)\n",
      "(234, 0.20827458926207398)\n",
      "(235, 0.1974754118105811)\n",
      "(236, 0.18724567983806817)\n",
      "(237, 0.177549403670334)\n",
      "(238, 0.1683639679122819)\n",
      "(239, 0.15966598801431253)\n",
      "(240, 0.15141431333345365)\n",
      "(241, 0.1435945743173437)\n",
      "(242, 0.13618433209541053)\n",
      "(243, 0.12916129986007124)\n",
      "(244, 0.12250349231379165)\n",
      "(245, 0.11619468704449262)\n",
      "(246, 0.11021728193636485)\n",
      "(247, 0.10454767544691726)\n",
      "(248, 0.0991740309564546)\n",
      "(249, 0.09407803425733127)\n",
      "(250, 0.08924726071975943)\n",
      "(251, 0.0846697873797472)\n",
      "(252, 0.08032805502050276)\n",
      "(253, 0.07621378329143796)\n",
      "(254, 0.07231090155492466)\n",
      "(255, 0.06860950596325652)\n",
      "(256, 0.06509976843931134)\n",
      "(257, 0.061772047739613575)\n",
      "(258, 0.058616085180731116)\n",
      "(259, 0.055623680645563764)\n",
      "(260, 0.05278742802056931)\n",
      "(261, 0.05009509500219055)\n",
      "(262, 0.04754174003191769)\n",
      "(263, 0.04512128264756125)\n",
      "(264, 0.04282418331521884)\n",
      "(265, 0.04064572336870972)\n",
      "(266, 0.03858015940849552)\n",
      "(267, 0.03662004666671194)\n",
      "(268, 0.03476029943433179)\n",
      "(269, 0.03299631519813266)\n",
      "(270, 0.03132264923750926)\n",
      "(271, 0.02973467670689741)\n",
      "(272, 0.028228289685522087)\n",
      "(273, 0.026799950352313397)\n",
      "(274, 0.025444126154253312)\n",
      "(275, 0.02415750057854394)\n",
      "(276, 0.022936214974865074)\n",
      "(277, 0.02177765529807453)\n",
      "(278, 0.02067802519842213)\n",
      "(279, 0.019634798720513293)\n",
      "(280, 0.018645027213875186)\n",
      "(281, 0.01770524951535982)\n",
      "(282, 0.01681331466358396)\n",
      "(283, 0.015966812570467448)\n",
      "(284, 0.015163469298204509)\n",
      "(285, 0.014400885789740197)\n",
      "(286, 0.013677553203494914)\n",
      "(287, 0.012990732760362708)\n",
      "(288, 0.012338564940118561)\n",
      "(289, 0.011719372236431325)\n",
      "(290, 0.011131607618322615)\n",
      "(291, 0.010573683283210471)\n",
      "(292, 0.010043985285482556)\n",
      "(293, 0.009541406597458288)\n",
      "(294, 0.009063964368016222)\n",
      "(295, 0.008610643739051549)\n",
      "(296, 0.00818022560448728)\n",
      "(297, 0.007771502071501308)\n",
      "(298, 0.00738346229954568)\n",
      "(299, 0.007015165503433212)\n",
      "(300, 0.006665383039355552)\n",
      "(301, 0.006333085724093435)\n",
      "(302, 0.006017522581111734)\n",
      "(303, 0.005717805820548525)\n",
      "(304, 0.0054332402116028765)\n",
      "(305, 0.0051629439105842995)\n",
      "(306, 0.00490637105255599)\n",
      "(307, 0.004662560491869961)\n",
      "(308, 0.00443093151586747)\n",
      "(309, 0.004210951493201616)\n",
      "(310, 0.004002027427917527)\n",
      "(311, 0.0038035401354298134)\n",
      "(312, 0.003615070330756035)\n",
      "(313, 0.003436025645932519)\n",
      "(314, 0.003265870494421505)\n",
      "(315, 0.0031042645075938947)\n",
      "(316, 0.00295067126655337)\n",
      "(317, 0.0028047725836082803)\n",
      "(318, 0.00266617900876006)\n",
      "(319, 0.0025345480143908958)\n",
      "(320, 0.0024094236661241614)\n",
      "(321, 0.0022905172276310878)\n",
      "(322, 0.0021775573220967543)\n",
      "(323, 0.002070218959351031)\n",
      "(324, 0.0019682088282200636)\n",
      "(325, 0.0018713124998841395)\n",
      "(326, 0.0017792325603505905)\n",
      "(327, 0.0016916762588572712)\n",
      "(328, 0.0016085014963733123)\n",
      "(329, 0.0015294271986488446)\n",
      "(330, 0.0014542828737164688)\n",
      "(331, 0.0013828749041027277)\n",
      "(332, 0.001315032982664193)\n",
      "(333, 0.0012505149518653633)\n",
      "(334, 0.001189200097554746)\n",
      "(335, 0.0011309087439573906)\n",
      "(336, 0.0010755130522836487)\n",
      "(337, 0.0010228389020970252)\n",
      "(338, 0.0009728067743595409)\n",
      "(339, 0.0009252122156727112)\n",
      "(340, 0.0008799697860604943)\n",
      "(341, 0.0008369565387106438)\n",
      "(342, 0.0007960681703824092)\n",
      "(343, 0.0007571918700323662)\n",
      "(344, 0.0007202468896300278)\n",
      "(345, 0.0006851142572056345)\n",
      "(346, 0.000651713452665634)\n",
      "(347, 0.0006199407746104795)\n",
      "(348, 0.000589736860591398)\n",
      "(349, 0.0005610133196097128)\n",
      "(350, 0.000533706620189809)\n",
      "(351, 0.0005077520453089487)\n",
      "(352, 0.00048305440664628475)\n",
      "(353, 0.00045956843845776845)\n",
      "(354, 0.00043723728416035794)\n",
      "(355, 0.00041599524148187275)\n",
      "(356, 0.0003958014979520958)\n",
      "(357, 0.0003766044609502257)\n",
      "(358, 0.0003583392173658662)\n",
      "(359, 0.000340963736031682)\n",
      "(360, 0.0003244356583351719)\n",
      "(361, 0.00030871765006880425)\n",
      "(362, 0.0002937664377743675)\n",
      "(363, 0.00027955160480498675)\n",
      "(364, 0.0002660284914751536)\n",
      "(365, 0.0002531600603383226)\n",
      "(366, 0.00024092069373817432)\n",
      "(367, 0.00022927785420613406)\n",
      "(368, 0.0002182009038612295)\n",
      "(369, 0.00020766915567789056)\n",
      "(370, 0.00019764882757396872)\n",
      "(371, 0.00018811481117103347)\n",
      "(372, 0.00017904115480064426)\n",
      "(373, 0.00017040988436255378)\n",
      "(374, 0.00016219723133007615)\n",
      "(375, 0.00015438558167554624)\n",
      "(376, 0.0001469552926752544)\n",
      "(377, 0.0001398816312354026)\n",
      "(378, 0.00013315137713832456)\n",
      "(379, 0.00012674759737477603)\n",
      "(380, 0.00012065377114124534)\n",
      "(381, 0.00011485624842778966)\n",
      "(382, 0.00010934223202715407)\n",
      "(383, 0.00010409181087109594)\n",
      "(384, 9.909539801456618e-05)\n",
      "(385, 9.43400076175721e-05)\n",
      "(386, 8.981580896089707e-05)\n",
      "(387, 8.550873861766612e-05)\n",
      "(388, 8.141327199978103e-05)\n",
      "(389, 7.7512822900572e-05)\n",
      "(390, 7.379998749087938e-05)\n",
      "(391, 7.026728971889415e-05)\n",
      "(392, 6.690387876411087e-05)\n",
      "(393, 6.37038207205054e-05)\n",
      "(394, 6.065871555971115e-05)\n",
      "(395, 5.775946748499297e-05)\n",
      "(396, 5.4999473236937125e-05)\n",
      "(397, 5.2371770083433006e-05)\n",
      "(398, 4.987110684794004e-05)\n",
      "(399, 4.7490225050602687e-05)\n",
      "(400, 4.522528076690612e-05)\n",
      "(401, 4.306845958287595e-05)\n",
      "(402, 4.1014463842603064e-05)\n",
      "(403, 3.905944247705796e-05)\n",
      "(404, 3.719803360526734e-05)\n",
      "(405, 3.54260697493198e-05)\n",
      "(406, 3.374003058256789e-05)\n",
      "(407, 3.213409359468797e-05)\n",
      "(408, 3.0605239859831144e-05)\n",
      "(409, 2.9149182631766225e-05)\n",
      "(410, 2.7762997633139686e-05)\n",
      "(411, 2.644318855267249e-05)\n",
      "(412, 2.518696528354721e-05)\n",
      "(413, 2.3990813580621976e-05)\n",
      "(414, 2.285137785538259e-05)\n",
      "(415, 2.1766619331610012e-05)\n",
      "(416, 2.0733508485889516e-05)\n",
      "(417, 1.9749782040909715e-05)\n",
      "(418, 1.881358441522863e-05)\n",
      "(419, 1.7921668534476e-05)\n",
      "(420, 1.7072328900480756e-05)\n",
      "(421, 1.626335402887311e-05)\n",
      "(422, 1.5492989862155818e-05)\n",
      "(423, 1.4759328666971155e-05)\n",
      "(424, 1.4060893095071151e-05)\n",
      "(425, 1.3395672965727286e-05)\n",
      "(426, 1.276185828509143e-05)\n",
      "(427, 1.2158308119049541e-05)\n",
      "(428, 1.1583440653557504e-05)\n",
      "(429, 1.103596499261979e-05)\n",
      "(430, 1.051478678925479e-05)\n",
      "(431, 1.0018169194564588e-05)\n",
      "(432, 9.545102597970576e-06)\n",
      "(433, 9.094462064874811e-06)\n",
      "(434, 8.665296174663055e-06)\n",
      "(435, 8.25645049437007e-06)\n",
      "(436, 7.867185134284155e-06)\n",
      "(437, 7.4963082125984e-06)\n",
      "(438, 7.1429018530652026e-06)\n",
      "(439, 6.80629867701302e-06)\n",
      "(440, 6.485626712425079e-06)\n",
      "(441, 6.180126414403228e-06)\n",
      "(442, 5.889335875314697e-06)\n",
      "(443, 5.612106803530023e-06)\n",
      "(444, 5.348028574292418e-06)\n",
      "(445, 5.096433005462166e-06)\n",
      "(446, 4.856716166912535e-06)\n",
      "(447, 4.628377487320796e-06)\n",
      "(448, 4.410938340991539e-06)\n",
      "(449, 4.203672932664405e-06)\n",
      "(450, 4.006152491580653e-06)\n",
      "(451, 3.817990800665471e-06)\n",
      "(452, 3.6387137196431414e-06)\n",
      "(453, 3.4678825041492984e-06)\n",
      "(454, 3.305253450650709e-06)\n",
      "(455, 3.150154565953301e-06)\n",
      "(456, 3.0023985019977195e-06)\n",
      "(457, 2.8616025348230788e-06)\n",
      "(458, 2.7274425185021023e-06)\n",
      "(459, 2.5996384370683374e-06)\n",
      "(460, 2.4778728607513206e-06)\n",
      "(461, 2.361810179677038e-06)\n",
      "(462, 2.251186746960943e-06)\n",
      "(463, 2.1457779432287743e-06)\n",
      "(464, 2.045333617013018e-06)\n",
      "(465, 1.9496143207779847e-06)\n",
      "(466, 1.8584475163831018e-06)\n",
      "(467, 1.771508133838292e-06)\n",
      "(468, 1.6886687058914045e-06)\n",
      "(469, 1.6097115621673028e-06)\n",
      "(470, 1.5344663675895455e-06)\n",
      "(471, 1.4627779170056197e-06)\n",
      "(472, 1.394458210689324e-06)\n",
      "(473, 1.329328060571626e-06)\n",
      "(474, 1.2672488875604896e-06)\n",
      "(475, 1.208083467136976e-06)\n",
      "(476, 1.1516957472313046e-06)\n",
      "(477, 1.0979628065611626e-06)\n",
      "(478, 1.0467563765796196e-06)\n",
      "(479, 9.979260380411192e-07)\n",
      "(480, 9.513885960243406e-07)\n",
      "(481, 9.070295582493153e-07)\n",
      "(482, 8.647498107820765e-07)\n",
      "(483, 8.244668116558777e-07)\n",
      "(484, 7.860568369769087e-07)\n",
      "(485, 7.494419508342832e-07)\n",
      "(486, 7.145362583196007e-07)\n",
      "(487, 6.812655760208169e-07)\n",
      "(488, 6.495499162496277e-07)\n",
      "(489, 6.193333947200114e-07)\n",
      "(490, 5.90517512443494e-07)\n",
      "(491, 5.63043149475871e-07)\n",
      "(492, 5.368551151922675e-07)\n",
      "(493, 5.118862946572499e-07)\n",
      "(494, 4.880911225167331e-07)\n",
      "(495, 4.6541509633069336e-07)\n",
      "(496, 4.437828960880367e-07)\n",
      "(497, 4.231628430001764e-07)\n",
      "(498, 4.0350120788472334e-07)\n",
      "(499, 3.847608665588136e-07)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension/\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Tensor\n",
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won’t be enough for modern deep learning.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the `Tensor`. A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors. Behind the scenes, Tensors can keep track of a computational graph and gradients, but they’re also useful as a generic tool for scientific computing.\n",
    "\n",
    "Also unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch Tensor on GPU, you simply need to cast it to a new datatype.\n",
    "\n",
    "Here we use PyTorch Tensors to fit a two-layer network to random data. Like the numpy example above we need to manually implement the forward and backward passes through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda: 0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # forward pass: compute predicted y\n",
    "    h = x.mm(w1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
